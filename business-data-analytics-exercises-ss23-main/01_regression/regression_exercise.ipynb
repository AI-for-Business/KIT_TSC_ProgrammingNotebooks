{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Data Analytics - Exercise Regression Tutorial\n",
    "\n",
    "This notebook is designed to illustrate the basic steps in a data science project. It is inspired by two notebooks from Kaggle, which is a platform that organises data science comptitions. Click [here](https://www.kaggle.com/code/abdelrahmantarek13/houseprice-step-by-step/notebook) and [here](https://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard) to see these notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CRISP-DM Process\n",
    "\n",
    "> Cross-industry standard process for data mining, also known as CRISP-DM, is an open standard process model that describes common approaches used by data mining experts. It is the most widely-used analytics model.\n",
    "> \n",
    "> -- Source: https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png\" width=\"400\" />\n",
    "</p>\n",
    "\n",
    "CRISP-DM breaks the process of data mining into six major phases:\n",
    "\n",
    "- Business Understanding\n",
    "- Data Understanding\n",
    "- Data Preparation\n",
    "- Modeling\n",
    "- Evaluation\n",
    "- Deployment\n",
    "\n",
    "The sequence of the phases is not strict and moving back and forth between different phases is usually required. The arrows in the process diagram indicate the most important and frequent dependencies between phases. The outer circle in the diagram symbolizes the cyclic nature of data mining itself. A data mining process continues after a solution has been deployed. The lessons learned during the process can trigger new, often more focused business questions, and subsequent data mining processes will benefit from the experiences of previous ones.\n",
    "\n",
    "**Disclaimer**: Because we are not solving a real-world data science project, we are skipping the **Business Understanding** and **Deployment Step**. However, in my experience, these steps are the most important ones to provide business value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description: House Prices - Advanced Regression Techniques\n",
    "\n",
    "This notebook follows the idea of the \"House Prices - Advanced Regression Techniques\" competition on Kaggle. However, the dataset for this competition has been compiled by Dean De Cock for use in data science education. It was designed after the Boston Housing dataset and is now considered a more modernized and expanded version of it. More details of this dataset are described in [Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project](http://jse.amstat.org/v19n3/decock.pdf).\n",
    "\n",
    ">**Goal**: It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n",
    ">\n",
    ">**Metric**: Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n",
    ">\n",
    "> -- description taken from [Kaggle](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview/evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install & import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from scipy import stats  # statistical functions\n",
    "import os  # access to operating system related functions\n",
    "\n",
    "# plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ml related libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from category_encoders.target_encoder import TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot inline\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "In the next cell, we will download the data from an url and differentiate between the features X and the target variable y. Then we will create a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download original data\n",
    "data = pd.read_csv(\"http://jse.amstat.org/v19n3/decock/AmesHousing.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define not needed columns\n",
    "non_feature_columns = ['PID', 'Order', 'SalePrice']\n",
    "\n",
    "# get features and target\n",
    "X, y = data.drop(non_feature_columns, axis=1), data['SalePrice']\n",
    "\n",
    "# split into train and testset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read more about the data\n",
    "with open('./data_description.txt', 'r') as file:\n",
    "    description = file.read()\n",
    "    \n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "### Gathering basic information about our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying first rows of data set\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get information about data types\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the numbers of samples and features\n",
    "print(\"The X_train data size is : {} \".format(X_train.shape))\n",
    "print(\"The X_test data size is : {} \".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting target variable\n",
    "\n",
    "Since we are interested in forecasting the house price, we will first have a look at the distribution of the house prices themselves. The first plot shows the distribution of the sales price, while the second plot shows the probability of our data against the quantiles of a specified theoretical distribution. If our target variable followed a (perfect) normal distribution, all blue points would be on the red line. For more information on the second plot, click [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize SalesPrice (target variable)\n",
    "sns.distplot(y_train , fit=stats.norm)\n",
    "\n",
    "#Now plot the distribution\n",
    "(mu, sigma) = stats.norm.fit(y_train)\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(y_train, plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: What are the conclusions that we can draw from these two plots?\n",
    "Use the cell below to answer this question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer goes here. Double click on cell to open it or create a new one with the \"plus\" symbol at the top navigation bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize features\n",
    "\n",
    "Now that we have a better understanding of what we are looking at, we can explore our features visually. One possible way of doing so is to create histograms of all features. Checkout the matplotlib and seaborn library for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualizing numerical features\n",
    "X_train.select_dtypes(np.number).hist(bins=50,figsize =(30,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## visualizing categorical data\n",
    "categorical_columns = X_train.select_dtypes('object').columns\n",
    "\n",
    "n_columns = 5\n",
    "n_rows = len(categorical_columns) // n_columns + 1\n",
    "\n",
    "fig = plt.figure(figsize =(20,30))\n",
    "\n",
    "for idx, column in enumerate(categorical_columns):\n",
    "    \n",
    "    ax = plt.subplot(n_rows, n_columns, idx + 1)\n",
    "    X_train[column].value_counts().plot(kind='bar')\n",
    "    ax.set_title(f'Distribution of {column}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize missing data ratio\n",
    "X_train_na = (X_train.isnull().sum() / len(X_train)) * 100\n",
    "X_train_na = X_train_na.drop(X_train_na[X_train_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio': X_train_na})\n",
    "missing_data.head(20)\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(x=X_train_na.index, y=X_train_na)\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize correlation\n",
    "sns.heatmap(X_train.select_dtypes(np.number).corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**: usually, one would conduct an even more in-depth visual analysis of the dataset. For instance, one would investigate the relationship between all variables and the target variable. The Python package [Seaborn](https://seaborn.pydata.org/index.html) provides some good tutorials on data visualisation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Below is a brief, non-exhaustive overview of the most common data preparation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values\n",
    "\n",
    "Imputing missing values often requires domain knowledge. In our dataset, for instance, there are a lot of columns, in which the missing value has a meaning and can therefore be meaningful encoded. If any value is missing at random, we can only make assumptions what this value should be encoded as. However, there are some advanced imputing techniques like k-nearest neighbors or an iterative imputer that try to make the best guess for us. If you want to read more about them, checkout sklearn's [documentation](https://scikit-learn.org/stable/modules/impute.html#impute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Please think about how to impute your missing values\n",
    "Hints: \n",
    "- You need to distinguish between categorical and numerical missing values. \n",
    "- Sometimes a missing value can be imputed by looking at the neighborhood. \n",
    "\n",
    "If you are stuck and don't know how to proceed, you can skip this step and use the Simple Imputer as below.\n",
    "\n",
    "Use the next cell below to for your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### encode all other features with either mode or median\n",
    "numeric_columns = X_train.select_dtypes(np.number).columns.to_list()\n",
    "categorical_columns = X_train.select_dtypes('object').columns.to_list()\n",
    "\n",
    "for column in numeric_columns:\n",
    "    numeric_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    X_train[column] = numeric_imputer.fit_transform(X_train[[column]])\n",
    "    X_test[column] = numeric_imputer.transform(X_test[[column]])\n",
    "\n",
    "for column in categorical_columns:\n",
    "    categorical_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    X_train[column] = categorical_imputer.fit_transform(X_train[[column]])\n",
    "    X_test[column] = categorical_imputer.transform(X_test[[column]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier removal\n",
    "\n",
    "Some models, like linear regression, is sensitive to outliers. Hence, depending on your models requirements, you might want to exclude abnormal data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Please investigate the above grade square feet area ('Gr Liv Area') for outliers\n",
    "Hint: A scatterplot might help.\n",
    "\n",
    "Use the cells below to for your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "In order to maximize our model's performance, we should also look into creating new features. This usually requires domain knowledge. However, there are also automated tools available. One of these tools is called featuretools. Click [here](https://github.com/alteryx/featuretools) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Create a new feature and visualize if it has any correlation with the target variable\n",
    "Hint: We can calculate a feature that describes the overall square feet of a house by adding the square feet of the basement with the ones of the first and second floor. The respective variables are called \"Total Bsmt SF\", \"1st Flr SF\" and \"2nd Flr SF\". You can use a scatterplot to visualize the relationship between the new feature and the target variable. \n",
    "\n",
    "Use the cells below to for your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of categorical features \n",
    "\n",
    "In sklearn, all machine learning algorithms assume that the categorical features are represented as numbers. This transformation can be done in many ways. Among the most popular is probably [one-hot-encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) or [label encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder). If you are interested in reading more about other, not so common encoding possibilities, check out the [category_encoder package](https://contrib.scikit-learn.org/category_encoders/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Please encode your categorical features as numbers. Also think about numeric variables that are actually categorical.\n",
    "Use the cells below to for your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming some numerical features that are actually categorical features\n",
    "for column in ['MS SubClass', 'Overall Cond', 'Yr Sold', 'Mo Sold']:    \n",
    "    X_train[column] = X_train[column].apply(str)\n",
    "    X_test[column] = X_test[column].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical features with target encoding\n",
    "categorical_columns = X_train.select_dtypes('object').columns.to_list()\n",
    "encoder = TargetEncoder(cols=categorical_columns)\n",
    "\n",
    "X_train = encoder.fit_transform(X=X_train, y=y_train)\n",
    "X_test = encoder.transform(X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform feature selection / extraction\n",
    "Usually, one would also perform feature selection or feature extraction. This will most likely increase the model performance if done well. Also, if you later want to interpret your model and its result, often feature selection is preferred. You can read more about feature selection [here](https://scikit-learn.org/stable/modules/feature_selection.html). \n",
    "\n",
    "For the sake of simplicity, we will skip this step here and deal with it in another exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming target variable\n",
    "\n",
    "As shown earlier, the distribution of the target variable is skewed. In some cases, it makes sense to transform the distribution to make it resemble a normal distribution. However, this is not always a good idea and depends a lot on the error metric. If you want to read more about transforming the target variable, read Florian Wilhelm's [blogpost](https://florianwilhelm.info/2020/05/honey_i_shrunk_the_target_variable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Please transform your target variable and plot its distribution afterwards\n",
    "\n",
    "Hint: You can use the logarithmic values of our target variable to make the distribution appear more normally distributed.\n",
    "\n",
    "Use the cells below to for your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start modelling, we need to think of a naive model. Without a naive model, we cannot put the performance of our machine learning model into perspective. A very simple naive model would be to take the mean of the sale prices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Please train at least two machine learning models.\n",
    "You are free to choose any model. An easy way to get started is to use models from [sklearn](https://scikit-learn.org/stable/index.html). Please also think of additional requirements that the model has with regard to the data, e.g. feature scaling. Which models are particularily suited for tabular data?\n",
    "\n",
    "Use the cells below to for your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Please evaluate your models on the given error metric. Are they better than the naive model?\n",
    "Hint: You can use one of sklearn's metric functions. See this [link](https://scikit-learn.org/stable/modules/model_evaluation.html) for more information.\n",
    "\n",
    "Use the cells below to for your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Please evaluate the residuals of your models. Do you consistently under- or overestimate the house prices?\n",
    "To plot the distribution of the residuals, you can use similar plots as used above. \n",
    "\n",
    "Use the cells below to for your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the residuals follow a normal distribution and hence we do not consistently under- or overestimate the house prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Think about how can you improve our existing model?\n",
    "\n",
    "Use the cell below to for your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve our model, we could do the following steps:\n",
    "\n",
    "- create better features, e.g. by having a closer look at the data\n",
    "- remove irrelevant features, e.g. by using recursive feature selection\n",
    "- create an ensemble to improve predictions\n",
    "- hyperparameter tuning\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2258dfa08c46b77d2bc2b7524b68bff9e582ce15fe6c25ecebf58bb0c8a8f397"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
